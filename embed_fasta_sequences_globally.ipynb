{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Gene names  (primary )</th>\n",
       "      <th>Entry name</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Features</th>\n",
       "      <th>...</th>\n",
       "      <th>Involvement in disease</th>\n",
       "      <th>Virus hosts</th>\n",
       "      <th>Catalytic activity</th>\n",
       "      <th>Tissue specificity</th>\n",
       "      <th>Developmental stage</th>\n",
       "      <th>Induction</th>\n",
       "      <th>Peptide</th>\n",
       "      <th>Propeptide</th>\n",
       "      <th>Temperature dependence</th>\n",
       "      <th>Interacts with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8V0N6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLG_HAV88</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>Human hepatitis A virus genotype IIB (isolate ...</td>\n",
       "      <td>2227</td>\n",
       "      <td>MNMSRQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...</td>\n",
       "      <td>251,728</td>\n",
       "      <td>ATP-binding;Capsid protein;Coiled coil;Covalen...</td>\n",
       "      <td>Active site (4); Chain (19); Coiled coil (1); ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Homo sapiens (Human) [TaxID: 9606]</td>\n",
       "      <td>CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q5Y944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLG_HAVCF</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>Human hepatitis A virus genotype IIA (isolate ...</td>\n",
       "      <td>2225</td>\n",
       "      <td>MNMSRQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...</td>\n",
       "      <td>251,415</td>\n",
       "      <td>ATP-binding;Capsid protein;Coiled coil;Covalen...</td>\n",
       "      <td>Active site (3); Chain (19); Coiled coil (1); ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Homo sapiens (Human) [TaxID: 9606]</td>\n",
       "      <td>CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q67825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLG_HAVGB</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>Human hepatitis A virus genotype IA (isolate G...</td>\n",
       "      <td>2227</td>\n",
       "      <td>MNMSKQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...</td>\n",
       "      <td>251,565</td>\n",
       "      <td>ATP-binding;Capsid protein;Coiled coil;Covalen...</td>\n",
       "      <td>Active site (3); Chain (19); Coiled coil (1); ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Homo sapiens (Human) [TaxID: 9606]</td>\n",
       "      <td>CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P08617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLG_HAVHM</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>Human hepatitis A virus genotype IB (isolate H...</td>\n",
       "      <td>2227</td>\n",
       "      <td>MNMSRQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...</td>\n",
       "      <td>251,508</td>\n",
       "      <td>3D-structure;ATP-binding;Capsid protein;Covale...</td>\n",
       "      <td>Active site (3); Beta strand (66); Chain (19);...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cercopithecus hamlyni (Owl-faced monkey) (Haml...</td>\n",
       "      <td>CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A5LGW7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLG_HAVJ8</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>Human hepatitis A virus genotype IIIB (isolate...</td>\n",
       "      <td>2228</td>\n",
       "      <td>MNMSRQGIFQTVGSGLDHILSLADVEEEQMIQSVDRTAVTGASYFT...</td>\n",
       "      <td>251,802</td>\n",
       "      <td>ATP-binding;Capsid protein;Coiled coil;Covalen...</td>\n",
       "      <td>Active site (3); Chain (19); Coiled coil (1); ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cercopithecus hamlyni (Owl-faced monkey) (Haml...</td>\n",
       "      <td>CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37397</th>\n",
       "      <td>Q9P2T1</td>\n",
       "      <td>GMPR2</td>\n",
       "      <td>GMPR2_HUMAN</td>\n",
       "      <td>GMP reductase 2 (GMPR 2) (EC 1.7.1.7) (Guanosi...</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>348</td>\n",
       "      <td>MPHIDNDVKLDFKDVLLRPKRSTLKSRSEVDLTRSFSFRNSKQTYS...</td>\n",
       "      <td>37,874</td>\n",
       "      <td>3D-structure;Acetylation;Alternative splicing;...</td>\n",
       "      <td>Active site (2); Alternative sequence (2); Bet...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CATALYTIC ACTIVITY: Reaction=IMP + NADP(+) + N...</td>\n",
       "      <td>TISSUE SPECIFICITY: Highly expressed in heart,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37398</th>\n",
       "      <td>Q8IUX8</td>\n",
       "      <td>EGFL6</td>\n",
       "      <td>EGFL6_HUMAN</td>\n",
       "      <td>Epidermal growth factor-like protein 6 (EGF-li...</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>553</td>\n",
       "      <td>MPLPWSLALPLLLSWVAGGFGNAASARHHGLLASARQPGVCHYGTK...</td>\n",
       "      <td>61,317</td>\n",
       "      <td>Alternative splicing;Basement membrane;Calcium...</td>\n",
       "      <td>Alternative sequence (1); Chain (1); Coiled co...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEVELOPMENTAL STAGE: Only detected in fetal ti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37399</th>\n",
       "      <td>O15372</td>\n",
       "      <td>EIF3H</td>\n",
       "      <td>EIF3H_HUMAN</td>\n",
       "      <td>Eukaryotic translation initiation factor 3 sub...</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>352</td>\n",
       "      <td>MASRKEGTGSTATSSSSTAGAAGKGKGKGGSGDSAVKQVQIDGLVV...</td>\n",
       "      <td>39,930</td>\n",
       "      <td>3D-structure;Cytoplasm;Initiation factor;Isope...</td>\n",
       "      <td>Beta strand (7); Chain (1); Compositional bias...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37400</th>\n",
       "      <td>P54852</td>\n",
       "      <td>EMP3</td>\n",
       "      <td>EMP3_HUMAN</td>\n",
       "      <td>Epithelial membrane protein 3 (EMP-3) (Hematop...</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>163</td>\n",
       "      <td>MSLLLLVVSALHILILILLFVATLDKSWWTLPGKESLNLWYDCTWN...</td>\n",
       "      <td>18,429</td>\n",
       "      <td>Glycoprotein;Membrane;Reference proteome;Trans...</td>\n",
       "      <td>Chain (1); Glycosylation (2); Natural variant ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37401</th>\n",
       "      <td>P50402</td>\n",
       "      <td>EMD</td>\n",
       "      <td>EMD_HUMAN</td>\n",
       "      <td>Emerin</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>254</td>\n",
       "      <td>MDNYADLSDTELTTLLRRYNIPHGPVVGSTRRLYEKKIFEYETQRR...</td>\n",
       "      <td>28,994</td>\n",
       "      <td>3D-structure;Acetylation;Actin-binding;Cardiom...</td>\n",
       "      <td>Chain (1); Domain (1); Helix (2); Modified res...</td>\n",
       "      <td>...</td>\n",
       "      <td>DISEASE: Emery-Dreifuss muscular dystrophy 1, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TISSUE SPECIFICITY: Skeletal muscle, heart, co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37402 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry Gene names  (primary )   Entry name  \\\n",
       "0      Q8V0N6                    NaN   POLG_HAV88   \n",
       "1      Q5Y944                    NaN   POLG_HAVCF   \n",
       "2      Q67825                    NaN   POLG_HAVGB   \n",
       "3      P08617                    NaN   POLG_HAVHM   \n",
       "4      A5LGW7                    NaN   POLG_HAVJ8   \n",
       "...       ...                    ...          ...   \n",
       "37397  Q9P2T1                  GMPR2  GMPR2_HUMAN   \n",
       "37398  Q8IUX8                  EGFL6  EGFL6_HUMAN   \n",
       "37399  O15372                  EIF3H  EIF3H_HUMAN   \n",
       "37400  P54852                   EMP3   EMP3_HUMAN   \n",
       "37401  P50402                    EMD    EMD_HUMAN   \n",
       "\n",
       "                                           Protein names  \\\n",
       "0      Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "1      Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "2      Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "3      Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "4      Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "...                                                  ...   \n",
       "37397  GMP reductase 2 (GMPR 2) (EC 1.7.1.7) (Guanosi...   \n",
       "37398  Epidermal growth factor-like protein 6 (EGF-li...   \n",
       "37399  Eukaryotic translation initiation factor 3 sub...   \n",
       "37400  Epithelial membrane protein 3 (EMP-3) (Hematop...   \n",
       "37401                                             Emerin   \n",
       "\n",
       "                                                Organism  Length  \\\n",
       "0      Human hepatitis A virus genotype IIB (isolate ...    2227   \n",
       "1      Human hepatitis A virus genotype IIA (isolate ...    2225   \n",
       "2      Human hepatitis A virus genotype IA (isolate G...    2227   \n",
       "3      Human hepatitis A virus genotype IB (isolate H...    2227   \n",
       "4      Human hepatitis A virus genotype IIIB (isolate...    2228   \n",
       "...                                                  ...     ...   \n",
       "37397                               Homo sapiens (Human)     348   \n",
       "37398                               Homo sapiens (Human)     553   \n",
       "37399                               Homo sapiens (Human)     352   \n",
       "37400                               Homo sapiens (Human)     163   \n",
       "37401                               Homo sapiens (Human)     254   \n",
       "\n",
       "                                                Sequence     Mass  \\\n",
       "0      MNMSRQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...  251,728   \n",
       "1      MNMSRQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...  251,415   \n",
       "2      MNMSKQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...  251,565   \n",
       "3      MNMSRQGIFQTVGSGLDHILSLADIEEEQMIQSVDRTAVTGASYFT...  251,508   \n",
       "4      MNMSRQGIFQTVGSGLDHILSLADVEEEQMIQSVDRTAVTGASYFT...  251,802   \n",
       "...                                                  ...      ...   \n",
       "37397  MPHIDNDVKLDFKDVLLRPKRSTLKSRSEVDLTRSFSFRNSKQTYS...   37,874   \n",
       "37398  MPLPWSLALPLLLSWVAGGFGNAASARHHGLLASARQPGVCHYGTK...   61,317   \n",
       "37399  MASRKEGTGSTATSSSSTAGAAGKGKGKGGSGDSAVKQVQIDGLVV...   39,930   \n",
       "37400  MSLLLLVVSALHILILILLFVATLDKSWWTLPGKESLNLWYDCTWN...   18,429   \n",
       "37401  MDNYADLSDTELTTLLRRYNIPHGPVVGSTRRLYEKKIFEYETQRR...   28,994   \n",
       "\n",
       "                                                Keywords  \\\n",
       "0      ATP-binding;Capsid protein;Coiled coil;Covalen...   \n",
       "1      ATP-binding;Capsid protein;Coiled coil;Covalen...   \n",
       "2      ATP-binding;Capsid protein;Coiled coil;Covalen...   \n",
       "3      3D-structure;ATP-binding;Capsid protein;Covale...   \n",
       "4      ATP-binding;Capsid protein;Coiled coil;Covalen...   \n",
       "...                                                  ...   \n",
       "37397  3D-structure;Acetylation;Alternative splicing;...   \n",
       "37398  Alternative splicing;Basement membrane;Calcium...   \n",
       "37399  3D-structure;Cytoplasm;Initiation factor;Isope...   \n",
       "37400  Glycoprotein;Membrane;Reference proteome;Trans...   \n",
       "37401  3D-structure;Acetylation;Actin-binding;Cardiom...   \n",
       "\n",
       "                                                Features  ...  \\\n",
       "0      Active site (4); Chain (19); Coiled coil (1); ...  ...   \n",
       "1      Active site (3); Chain (19); Coiled coil (1); ...  ...   \n",
       "2      Active site (3); Chain (19); Coiled coil (1); ...  ...   \n",
       "3      Active site (3); Beta strand (66); Chain (19);...  ...   \n",
       "4      Active site (3); Chain (19); Coiled coil (1); ...  ...   \n",
       "...                                                  ...  ...   \n",
       "37397  Active site (2); Alternative sequence (2); Bet...  ...   \n",
       "37398  Alternative sequence (1); Chain (1); Coiled co...  ...   \n",
       "37399  Beta strand (7); Chain (1); Compositional bias...  ...   \n",
       "37400  Chain (1); Glycosylation (2); Natural variant ...  ...   \n",
       "37401  Chain (1); Domain (1); Helix (2); Modified res...  ...   \n",
       "\n",
       "                                  Involvement in disease  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "37397                                                NaN   \n",
       "37398                                                NaN   \n",
       "37399                                                NaN   \n",
       "37400                                                NaN   \n",
       "37401  DISEASE: Emery-Dreifuss muscular dystrophy 1, ...   \n",
       "\n",
       "                                             Virus hosts  \\\n",
       "0                     Homo sapiens (Human) [TaxID: 9606]   \n",
       "1                     Homo sapiens (Human) [TaxID: 9606]   \n",
       "2                     Homo sapiens (Human) [TaxID: 9606]   \n",
       "3      Cercopithecus hamlyni (Owl-faced monkey) (Haml...   \n",
       "4      Cercopithecus hamlyni (Owl-faced monkey) (Haml...   \n",
       "...                                                  ...   \n",
       "37397                                                NaN   \n",
       "37398                                                NaN   \n",
       "37399                                                NaN   \n",
       "37400                                                NaN   \n",
       "37401                                                NaN   \n",
       "\n",
       "                                      Catalytic activity  \\\n",
       "0      CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...   \n",
       "1      CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...   \n",
       "2      CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...   \n",
       "3      CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...   \n",
       "4      CATALYTIC ACTIVITY: Reaction=a ribonucleoside ...   \n",
       "...                                                  ...   \n",
       "37397  CATALYTIC ACTIVITY: Reaction=IMP + NADP(+) + N...   \n",
       "37398                                                NaN   \n",
       "37399                                                NaN   \n",
       "37400                                                NaN   \n",
       "37401                                                NaN   \n",
       "\n",
       "                                      Tissue specificity  \\\n",
       "0                                                    NaN   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "37397  TISSUE SPECIFICITY: Highly expressed in heart,...   \n",
       "37398                                                NaN   \n",
       "37399                                                NaN   \n",
       "37400                                                NaN   \n",
       "37401  TISSUE SPECIFICITY: Skeletal muscle, heart, co...   \n",
       "\n",
       "                                     Developmental stage Induction Peptide  \\\n",
       "0                                                    NaN       NaN     NaN   \n",
       "1                                                    NaN       NaN     NaN   \n",
       "2                                                    NaN       NaN     NaN   \n",
       "3                                                    NaN       NaN     NaN   \n",
       "4                                                    NaN       NaN     NaN   \n",
       "...                                                  ...       ...     ...   \n",
       "37397                                                NaN       NaN     NaN   \n",
       "37398  DEVELOPMENTAL STAGE: Only detected in fetal ti...       NaN     NaN   \n",
       "37399                                                NaN       NaN     NaN   \n",
       "37400                                                NaN       NaN     NaN   \n",
       "37401                                                NaN       NaN     NaN   \n",
       "\n",
       "      Propeptide Temperature dependence Interacts with  \n",
       "0            NaN                    NaN              0  \n",
       "1            NaN                    NaN              0  \n",
       "2            NaN                    NaN              0  \n",
       "3            NaN                    NaN              0  \n",
       "4            NaN                    NaN              0  \n",
       "...          ...                    ...            ...  \n",
       "37397        NaN                    NaN              1  \n",
       "37398        NaN                    NaN              1  \n",
       "37399        NaN                    NaN             11  \n",
       "37400        NaN                    NaN             49  \n",
       "37401        NaN                    NaN             89  \n",
       "\n",
       "[37402 rows x 38 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"meta_human_Viruses_SWP_v2.tsv.gz\",sep=\"\\t\",low_memory=False).dropna(thresh=40,axis=1)\n",
    "df.drop(columns=[\"Status\",\"Gene names\"],axis=1,errors=\"ignore\",inplace=True)\n",
    "df[\"Interacts with\"] = df[\"Interacts with\"].str.split(\";\").str.len().fillna(0).astype(int)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Entry', 'Gene names  (primary )', 'Entry name', 'Protein names',\n",
       "       'Organism', 'Length', 'Sequence', 'Mass', 'Keywords', 'Features',\n",
       "       'Gene ontology (GO)', 'Protein families', 'Signal peptide',\n",
       "       'RNA editing', 'EC number', 'Taxonomic lineage (KINGDOM)',\n",
       "       'Taxonomic lineage (CLASS)', 'Taxonomic lineage (FAMILY)',\n",
       "       'Polymorphism', 'DNA binding', 'Cofactor', 'Binding site',\n",
       "       'Calcium binding', 'Activity regulation', 'pH dependence', 'Pathway',\n",
       "       'Pharmaceutical use', 'Biotechnological use', 'Involvement in disease',\n",
       "       'Virus hosts', 'Catalytic activity', 'Tissue specificity',\n",
       "       'Developmental stage', 'Induction', 'Peptide', 'Propeptide',\n",
       "       'Temperature dependence', 'Interacts with'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36424\n",
      "36904\n"
     ]
    }
   ],
   "source": [
    "print(df.drop_duplicates([\"Sequence\"]).shape[0])\n",
    "print(df.drop_duplicates([\"Sequence\",\"Keywords\",\"Virus hosts\",'Taxonomic lineage (FAMILY)']).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37402\n",
      "36904\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "df = df.drop_duplicates([\"Sequence\",\"Keywords\",\"Virus hosts\",'Taxonomic lineage (FAMILY)'])\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5w39f_fnY9"
   },
   "source": [
    "# Colab initialization\n",
    "- install the pipeline in the colab runtime\n",
    "- download files neccessary for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kalvvWZZgM0E",
    "outputId": "2efbb2cb-829b-4881-a088-187705a287a4"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYm9gzOufnZA",
    "outputId": "33af4924-76f7-4dbc-ae7b-1a3aa9991693"
   },
   "outputs": [],
   "source": [
    "# # !pip3 install -U pip > /dev/null\n",
    "# # !pip3 install -U bio_embeddings[all] > /dev/null\n",
    "\n",
    "# !pip install -U tqdm bio_embeddings[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_cRmHN0j709"
   },
   "source": [
    "* Use the unirpot fasta file I downloaded and uploaded to my drive\n",
    "\n",
    "`/content/drive/MyDrive/Research/biodata/proteins/Transmembrane_human_90.fasta`\n",
    "\n",
    "* Download fasta from: `https://www.uniprot.org/uniref/?query=uniprot:(keyword%3A%22Transmembrane+%5BKW-0812%5D%22+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B9606%5D%22)+identity:0.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwgVw-CN1OgJ",
    "outputId": "6e603b51-5b22-4a15-9394-0ffb98d060e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/protein/'\n",
      "C:\\Users\\Dan Ofer\\Desktop\\Stuff\\Research\\datasets\\anomalies\n"
     ]
    }
   ],
   "source": [
    "# cd  \"/content/drive/MyDrive/protein/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vm7f6oH00O4_",
    "outputId": "b6a54249-5649-4fd6-a4c0-f5674d31d59d"
   },
   "outputs": [],
   "source": [
    "# !wget http://data.bioembeddings.com/public/embeddings/notebooks/custom_data/tiny_sampled.fasta --output-document tiny_sampled.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZrxD5vg90QoW"
   },
   "outputs": [],
   "source": [
    "# MY_FASTA_PATH = \"/content/drive/MyDrive/protein/tiny_sampled.fasta\"\n",
    "# MY_FASTA_PATH = \"/content/drive/MyDrive/Research/biodata/proteins/Transmembrane_human_90.fasta\" # 14k sequences, with fragments, 90% id redundnancy\n",
    "\n",
    "# MY_FASTA_PATH = \"/content/drive/MyDrive/Research/biodata/proteins/Transmembrane_human_whole_100.fasta\" # 30K sequences, without fragments\n",
    "\n",
    "\n",
    "# MY_FASTA_PATH = \"Transmembrane_human_whole_100.fasta\"\n",
    "MY_FASTA_PATH =\"SWP_human_viruses_all.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa-7WUuSfnZC"
   },
   "source": [
    "# Embed sequences in a FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKw7JD3_fnZD",
    "outputId": "e334f65a-82a9-44a2-e6fb-dff44647b0fa"
   },
   "outputs": [],
   "source": [
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder, ProtTransAlbertBFDEmbedder#,\n",
    "from Bio import SeqIO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YtGmgYqQfnZE"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "# for record in SeqIO.parse(\"tiny_sampled.fasta\", \"fasta\"): # ORIG from demo - tiny file\n",
    "for record in SeqIO.parse(MY_FASTA_PATH, \"fasta\"): # run on all our data\n",
    "    sequences.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uIiZKlWtjbo",
    "outputId": "cf55cb81-de82-4365-fd50-5e6c5ece0701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37400\n",
      "37239\n",
      "37400\n"
     ]
    }
   ],
   "source": [
    "# sequences ## seqrecord item\n",
    "\n",
    "print(len(sequences))\n",
    "print(len([s for s in sequences if \"X\" not in str(s.seq)]))\n",
    "\n",
    "# # ## drop sequences with unknown AA / X  - OPTIONAL\n",
    "# sequences = [s for s in sequences if \"X\" not in str(s.seq)]\n",
    "\n",
    "random.shuffle(sequences)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OGc7FtstZxnu"
   },
   "outputs": [],
   "source": [
    "# sequences\n",
    "## sample ; id='UniRef90_A0A5F5Q3C8', name='UniRef90_A0A5F5Q3C8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ln7NwNmVZy8j"
   },
   "outputs": [],
   "source": [
    "# sequences = sequences[0:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWVA7azNfnZE",
    "outputId": "0c5cfd8c-c93d-4bbf-d222-fc6826247e05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./prottrans_albert_bfd were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.bias', 'sop_classifier.classifier.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'sop_classifier.classifier.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# embedder = ProtTransBertBFDEmbedder(half_model=True,half_precision_model=True)#)\n",
    "embedder = ProtTransAlbertBFDEmbedder(half_model=True,model_directory=\"./prottrans_albert_bfd\",half_precision_model=True)\n",
    "\n",
    "# embedder = SeqVecEmbedder(half_model=True) ## slightly smaller, faster\n",
    "# embedder = Word2VecEmbedder() ## far far faster. 512 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBD8M1OjfnZF",
    "outputId": "15f03204-aca6-47d6-cf21-5076615f8eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ## ORIG\n",
    "# embeddings = embedder.embed_many([str(s.seq) for s in sequences]) # ORIG\n",
    "\n",
    "# embeddings = embedder.embed_many([str(s.seq)[0:256] for s in sequences]) # Workaround for long sequences\n",
    "\n",
    "# # `embed_many` returns a generator.\n",
    "# # We want to keep both RAW embeddings and reduced embeddings in memory.\n",
    "# # To do so, we simply turn the generator into a list!\n",
    "# # (this will start embedding the sequences!)\n",
    "\n",
    "# # embeddings = list(embeddings) ## embeddings per position\n",
    "\n",
    "# reduced_embeddings = [ProtTransBertBFDEmbedder.reduce_per_protein(e) for e in embeddings] ## embeddings over whole sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tmAwPCufnZG",
    "outputId": "0a65c4ff-279f-43d6-abfb-4055ef4699cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 12 min for 4k rows, max 300 length\n",
    "# ### new/ALT - get reduced embeddings only, without batch? # - requires embed first\n",
    "\n",
    "# reduced_embeddings = [ProtTransBertBFDEmbedder.reduce_per_protein(s) for str(s.seq)[0:256] in sequences]\n",
    "\n",
    "embeddings = embedder.embed_many([str(s.seq)[0:4600] for s in sequences]) ## embeddings per position # truncated for Workaround for long sequences\n",
    "\n",
    "\n",
    "# embeddings = embedder.embed_many([str(s.seq)[0:500] for s in tqdm(sequences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c2f751df7d3d49768936d0c6a934daee",
      "3204517c958d4f5c9aa6e935605e94c0",
      "2667d1c5f50943a4b804505f060dd9bc",
      "79f8cfa4b0c746cb9396ba3dbfb9f599",
      "319dd78c1e9e48bf9bb7d8dbfc7fa9ed",
      "32dd959b359b46359b21ec93235a41a5",
      "c811afad65f74a849f25555ac37238ab",
      "60ff0fd1d50940b2b8f91c9ff1ce95ab",
      "28d546261b934630be8c531cc7881c05",
      "34c4daf9764c4b9ebdc853214970ea1b",
      "4cf4f0478f8f4e548a2a6b5a748a3822"
     ]
    },
    "id": "tao_n1o5rVjH",
    "outputId": "92a76309-fdd9-4c62-db2a-14e34c8a572c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eee5ea5b8a84b888b89ea061c8dda6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RuntimeError for sequence with 4600 residues: CUDA out of memory. Tried to allocate 2.53 GiB (GPU 0; 6.00 GiB total capacity; 3.08 GiB already allocated; 662.06 MiB free; 3.75 GiB reserved in total by PyTorch). This most likely means that you don't have enough GPU RAM to embed a protein this long. Embedding on the CPU instead, which is very slow\n",
      "Some weights of the model checkpoint at ./prottrans_albert_bfd were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.bias', 'sop_classifier.classifier.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'sop_classifier.classifier.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\bio_embeddings\\embed\\embedder_interfaces.py\u001b[0m in \u001b[0;36membed_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embed_batch_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\bio_embeddings\\embed\\prottrans_base_embedder.py\u001b[0m in \u001b[0;36m_embed_batch_impl\u001b[1;34m(self, batch, model)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             embeddings = model(\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenized_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    721\u001b[0m         )\n\u001b[1;32m--> 722\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    723\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             layer_group_output = self.albert_layer_groups[group_idx](\n\u001b[0m\u001b[0;32m    469\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malbert_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malbert_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malbert_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    387\u001b[0m     ):\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.53 GiB (GPU 0; 6.00 GiB total capacity; 3.08 GiB already allocated; 662.06 MiB free; 3.75 GiB reserved in total by PyTorch)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\bio_embeddings\\embed\\embedder_interfaces.py\u001b[0m in \u001b[0;36membed_many\u001b[1;34m(self, sequences, batch_size)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\bio_embeddings\\embed\\prottrans_base_embedder.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sequence)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\bio_embeddings\\embed\\embedder_interfaces.py\u001b[0m in \u001b[0;36membed_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    182\u001b[0m                     \u001b[1;34mf\"Embedding on the CPU instead, which is very slow\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 )\n\u001b[1;32m--> 184\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embed_batch_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_fallback_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 logger.error(\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\bio_embeddings\\embed\\prottrans_base_embedder.py\u001b[0m in \u001b[0;36m_embed_batch_impl\u001b[1;34m(self, batch, model)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             embeddings = model(\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenized_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             )\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         )\n\u001b[1;32m--> 722\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    723\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[0mgroup_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_groups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             layer_group_output = self.albert_layer_groups[group_idx](\n\u001b[0m\u001b[0;32m    469\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malbert_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malbert_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malbert_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     ):\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         ffn_output = apply_chunking_to_forward(\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mmixed_query_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[0mmixed_key_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mmixed_value_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ## try maybe embedder insyead of ProtTransBertBFDEmbedder ??\n",
    "# reduced_embeddings = [ProtTransBertBFDEmbedder.reduce_per_protein(e) for e in embeddings]  ## embeddings over whole sequence # ORI\n",
    "\n",
    "### ALT : [embedder.reduce_per_protein(embedding) for embedding in embedder.embed_many(data['HEAVY CDR3 (aa)'])]\n",
    "# reduced_embeddings = [embedder.reduce_per_protein(e) for e in tqdm.tqdm(embeddings)]  ## embeddings over whole sequence # ORI\n",
    "\n",
    "reduced_embeddings = [embedder.reduce_per_protein(e) for e in tqdm(embeddings)]  ## embeddings over whole sequence # ORI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cewa5rzSfnZI"
   },
   "outputs": [],
   "source": [
    "# for (per_amino_acid, per_protein) in zip(embeddings[0:19], reduced_embeddings[0:19]):\n",
    "#     print(per_amino_acid.shape, per_protein.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHjN1kR_8nAq"
   },
   "outputs": [],
   "source": [
    "# reduced_embeddings = [ProtTransBertBFDEmbedder.reduce_per_protein(e) for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8gsX8vNylgA"
   },
   "outputs": [],
   "source": [
    "print(len(reduced_embeddings))\n",
    "print(reduced_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7g72WpB8koYA"
   },
   "outputs": [],
   "source": [
    "reduced_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UlIHrEAt8yN"
   },
   "outputs": [],
   "source": [
    "ids = [str(s.id) for s in sequences]\n",
    "print(len(ids))\n",
    "ids[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aErMaZOJui2u"
   },
   "outputs": [],
   "source": [
    "descriptions = [str(s.description).replace(\" <unknown description>\",\"\") for s in sequences]\n",
    "print(len(descriptions))\n",
    "descriptions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edaen8N0uPx-"
   },
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(data={\"ID\":ids,\"descriptions\":descriptions})\n",
    "df_meta[\"ID\"] = df_meta[\"ID\"].str.split(\"_\",expand=True,n=1)[1].str.strip() ## get uniprot id, instead of uniref90_+uniprot ID\n",
    "\n",
    "## add some text features\n",
    "df_meta[\"fragment\"] = (df_meta[\"descriptions\"].str.contains(\"Fragment\",na=False)).astype(\"int\")\n",
    "df_meta[\"Unknown\"] = (df_meta[\"descriptions\"].str.contains(\"Uncharacterized|Unknown\",case=False)).astype(\"int\")\n",
    "df_meta[\"word_count\"] = df_meta[\"descriptions\"].str.split().str.len()\n",
    "df_meta[\"length\"] = [len(str(s.seq)) for s in sequences]\n",
    "print(df_meta.describe().round(2))\n",
    "# df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eggiq94CubAw"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=reduced_embeddings) # numpy to pandas\n",
    " ## lengths of all the seqs\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjUGxT6Uvd3c"
   },
   "outputs": [],
   "source": [
    "assert df_meta.shape[0]==df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWPyqIEYvR4j"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_meta,df],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0jh-qjYxQut"
   },
   "source": [
    "#### Save output as parquet file to disc/drive (local runtime)\n",
    "* Can also save as csv if wanted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKSxqlESwMjW"
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.astype(str)\n",
    "df.to_parquet(\"membrane_100_whole_protbert_embed.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg2-xF7klqOh"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"membrane_100_whole_protbert_embed.csv.gz\",index=False,compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCl_Po1ZxnWu"
   },
   "source": [
    "## run anomaly detection models\n",
    "\n",
    "* https://scikit-learn.org/stable/auto_examples/neighbors/plot_lof_novelty_detection.html#sphx-glr-auto-examples-neighbors-plot-lof-novelty-detection-py\n",
    "\n",
    "* https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html#sphx-glr-auto-examples-miscellaneous-plot-anomaly-comparison-py\n",
    "\n",
    "* Could also do clustering\n",
    "\n",
    "* We will evaluate outcomes using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D-VdktNxm9u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYVErlIVx7V9"
   },
   "outputs": [],
   "source": [
    "X = reduced_embeddings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKytBh6Wxmyz"
   },
   "outputs": [],
   "source": [
    "clf = LocalOutlierFactor(n_neighbors=20,n_jobs=-2) # ,metric=\"cosine\"\n",
    "preds = clf.fit_predict(X)\n",
    "print(preds.mean(),\"mean preds\")\n",
    "\n",
    "df[\"anomaly_score_nn\"] = preds\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hofqtlIExmwE"
   },
   "outputs": [],
   "source": [
    "clf.negative_outlier_factor_\n",
    "\n",
    "# negative_outlier_factor_ndarray of shape (n_samples,)\n",
    "# The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (negative_outlier_factor_ close to -1), while outliers tend to have a larger LOF score.\n",
    "\n",
    "# The local outlier factor (LOF) of a sample captures its supposed â€˜degree of abnormalityâ€™. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb8SOvEKfhHP"
   },
   "outputs": [],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qm4pseREfhKG"
   },
   "outputs": [],
   "source": [
    "clf_iso = IsolationForest(max_samples=1500, max_features=0.9,n_jobs=-1,bootstrap=True)\n",
    "clf_iso.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNk_4WN0hKws"
   },
   "outputs": [],
   "source": [
    "##  The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.\n",
    "df[\"anomaly_score_iso\"] = clf_iso.decision_function(X) # get averaged, probalistic\" score\n",
    "df[[\"anomaly_score_iso\",\"anomaly_score_nn\"]].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgVzZShOhKZP"
   },
   "outputs": [],
   "source": [
    "df2 = df[[\"ID\",\"descriptions\",\"length\",\"fragment\",\"Unknown\",\"word_count\",\"anomaly_score_iso\",\"anomaly_score_nn\"]].sort_values([\"anomaly_score_iso\"]).copy()\n",
    "## FLIP direction so that higher is more anomalous, at least for iso_forest, for easier interpretation/correlation\n",
    "\n",
    "df2[\"anomaly_score_iso\"] = -1*df2[\"anomaly_score_iso\"]\n",
    "df2[\"anomaly_score_nn\"] = -1*df2[\"anomaly_score_nn\"]\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AEl3Ahini9g"
   },
   "source": [
    "#### Top rows = predict to be most anomalous (by isolation forest model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXJuNI_4hfTs"
   },
   "outputs": [],
   "source": [
    "df2.loc[df2[\"anomaly_score_nn\"]==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ID7WPYrQiiTj"
   },
   "outputs": [],
   "source": [
    "## check correlation of the 2 models - how many mutual/shared anomalous predictions?\n",
    "df2.loc[(df2[\"anomaly_score_nn\"]==1) & (df2[\"anomaly_score_iso\"]>=0)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8UO16UcjXJb"
   },
   "outputs": [],
   "source": [
    "list(df2.descriptions.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lI0M3wHBj6gy"
   },
   "outputs": [],
   "source": [
    "## check whether fragment is common\n",
    "df2.loc[df2[\"descriptions\"].str.contains(\"Fragment\")].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ_QPuEPkm34"
   },
   "outputs": [],
   "source": [
    "df2.corr().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWYqxkJS0XM7"
   },
   "outputs": [],
   "source": [
    "print(df2.loc[(df2[\"anomaly_score_nn\"]==1) & (df2[\"anomaly_score_iso\"]>=0.05)].shape[0])\n",
    "df2.loc[(df2[\"anomaly_score_nn\"]==1) & (df2[\"anomaly_score_iso\"]>=0.05)][\"ID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JIX9tXDksYK"
   },
   "outputs": [],
   "source": [
    "df2.head(50)[\"ID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYgYagBR6Q9Q"
   },
   "outputs": [],
   "source": [
    "df2.tail(500)[\"ID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9j1WxNke7xEl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "embed_membrane_sequences_globally_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2667d1c5f50943a4b804505f060dd9bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c811afad65f74a849f25555ac37238ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_32dd959b359b46359b21ec93235a41a5",
      "value": ""
     }
    },
    "28d546261b934630be8c531cc7881c05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "319dd78c1e9e48bf9bb7d8dbfc7fa9ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cf4f0478f8f4e548a2a6b5a748a3822",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_34c4daf9764c4b9ebdc853214970ea1b",
      "value": " 12709/? [51:46&lt;00:00,  4.38it/s]"
     }
    },
    "3204517c958d4f5c9aa6e935605e94c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32dd959b359b46359b21ec93235a41a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34c4daf9764c4b9ebdc853214970ea1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cf4f0478f8f4e548a2a6b5a748a3822": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60ff0fd1d50940b2b8f91c9ff1ce95ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "79f8cfa4b0c746cb9396ba3dbfb9f599": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28d546261b934630be8c531cc7881c05",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_60ff0fd1d50940b2b8f91c9ff1ce95ab",
      "value": 1
     }
    },
    "c2f751df7d3d49768936d0c6a934daee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2667d1c5f50943a4b804505f060dd9bc",
       "IPY_MODEL_79f8cfa4b0c746cb9396ba3dbfb9f599",
       "IPY_MODEL_319dd78c1e9e48bf9bb7d8dbfc7fa9ed"
      ],
      "layout": "IPY_MODEL_3204517c958d4f5c9aa6e935605e94c0"
     }
    },
    "c811afad65f74a849f25555ac37238ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
