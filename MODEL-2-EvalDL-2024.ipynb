{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5w39f_fnY9"
   },
   "source": [
    "esm based dynamic model (not using static embeds).\n",
    "\n",
    "+ Use HF Trainer, LORA:\n",
    "  * https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "\n",
    "Use TF:\n",
    "*  üá∞https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb#scrollTo=de8419b5\n",
    "* Torch based /Trainer example:  https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=49dcba23\n",
    "\n",
    "\n",
    "\n",
    "* NOte for pytorch training could use trainer maybe, and mixed precision - https://huggingface.co/docs/transformers/v4.18.0/en/performance#fp16-training\n",
    "\n",
    "\n",
    "* QLORA finetuning: https://huggingface.co/blog/AmelieSchreiber/esm2-ptm\n",
    "  * https://huggingface.co/blog/AmelieSchreiber/esmbind   (token level)\n",
    "\n",
    "* Another lora, qlora example - may use too much mem/bug : https://github.com/huggingface/peft/issues/1023\n",
    "* Default trainer (`AutoModelForSequenceClassification`) + Lora https://huggingface.co/docs/peft/task_guides/image_classification_lora\n",
    "   * seq cls with lora - maybe `task_type=\"SEQ_CLS\"` ? https://github.com/huggingface/peft/blob/main/docs/source/task_guides/ptuning-seq-classification.md\n",
    "* https://www.kaggle.com/code/andregrbnr/protein-sequence-classification - lora modules to save ??\n",
    "\n",
    "  * ESM2-Lora mem bug (also accel data loop) ? https://github.com/huggingface/peft/issues/1023\n",
    "\n",
    "\n",
    "* QLORA: https://huggingface.co/blog/AmelieSchreiber/esm2-ptm\n",
    "  * `36 batch size` with esm-150M !\n",
    "\n",
    "* lora peft - classifier layer weight saving issue?  https://github.com/huggingface/peft/issues/577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kalvvWZZgM0E"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GO4tbwSU1pa9"
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge google-colab -y\n",
    "\n",
    "# !pip install tensorflow  -U -q # ankh\n",
    "\n",
    "# !pip install torch  -U -q # fair-esm # seqeval\n",
    "# !pip install transformers peft accelerate datasets evaluate bitsandbytes -U -q # --user\n",
    "# !pip install peft bitsandbytes -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_cRmHN0j709"
   },
   "source": [
    "* Use the unirpot fasta file I downloaded and uploaded to my drive\n",
    "\n",
    "`/content/drive/MyDrive/Research/biodata/proteins/Transmembrane_human_90.fasta`\n",
    "\n",
    "* Download fasta from: `https://www.uniprot.org/uniref/?query=uniprot:(keyword%3A%22Transmembrane+%5BKW-0812%5D%22+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B9606%5D%22)+identity:0.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kwgVw-CN1OgJ"
   },
   "outputs": [],
   "source": [
    "#### DATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/swp_human_viri_all_embed_esm.parquet\" ## ESM1B embedding (max len 1022)\n",
    "# DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-train.csv.gz\"## TRAIN\n",
    "# DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "# DATA_PATH = \"/kaggle/input/humvir-proteins/hum_vir_swp-globalEmbed-train.csv/hum_vir_swp-globalEmbed-train.csv\"\n",
    "DATA_PATH = \"hum_vir_swp-globalEmbed-train.csv.gz\"\n",
    "\n",
    "## TEST data:\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/hum_vir_swp-globalEmbed-test.csv.gz\"## TRAIN\n",
    "# TEST_DATA_PATH = \"/content/drive/MyDrive/hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "TEST_DATA_PATH = \"hum_vir_swp-globalEmbed-test.csv.gz\"\n",
    "\n",
    "# ## metadata for all reviewed/swissprot human + virus proteins\n",
    "# METADATA_PATH = \"/content/drive/MyDrive/Research/CIDR-Protein Anomalies project/protein_anomalies_data/SWP_human_viruses_all.xlsx\"\n",
    "\n",
    "TARGET_COL = \"virus\" ## use for filtering data into 1 class\n",
    "\n",
    "MAX_LEN = 1024#768#530#480 # exclude sequences longer than this. (Not merely truncate)\n",
    "\n",
    "FAST_RUN = False#True\n",
    "SAVE_MODEL = False#True\n",
    "SAVE_OUTPUTS = False#True\n",
    "\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/content/drive/MyDrive/proteins/New Protein-Virus anom project/trained_esm_lora_trainer_model\"\n",
    "MODEL_DRIVE_SAVE_PATH = \"small_trained_esm_lora_trainer_model\"\n",
    "# MODEL_DRIVE_SAVE_PATH = \"/kaggle/input/humvir-proteins/qlora/qlora\" # saved + reuploadedon kaggle\n",
    "\n",
    "TRAIN_MODEL = True#False\n",
    "LOAD_TRAINED = False#True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa-7WUuSfnZC"
   },
   "source": [
    "# Embed sequences in a FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11612,
     "status": "ok",
     "timestamp": 1706644666124,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "hKw7JD3_fnZD",
    "outputId": "ffa04bad-8a62-447c-f1cd-466ebec4c602"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93/2867410804.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2024-03-12 09:45:21.262796: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-12 09:45:21.439838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-12 09:45:21.439866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-12 09:45:21.469938: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-12 09:45:21.530629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 09:45:22.367663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "# from Bio import SeqIO\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing  import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "# from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "## https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "# from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "# import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "# from transformers import AutoTokenizer #DataCollatorForLanguageModeling,\n",
    "## https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForSequenceClassification, AutoPeftModel\n",
    "\n",
    "## could use transformer pipeline for inference;\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "# from tqdm.auto import tqdm\n",
    "# pipe = pipeline(\"text-classification\", model=\"facebook/wav2vec2-base-960h\", device=0)\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"humVir_evalDLTest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yZRBksiPvHJ7"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(mode=\"disabled\")\n",
    "# # alt\n",
    "# wandb.init(project='qlora_humvir')# ; or args = TrainingArguments(report_to=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r8gFb91iUc8N"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "# # Use the accelerator\n",
    "# ### try disabling? (with qlora)\n",
    "# # accelerator = Accelerator()# trying this\n",
    "# # ◊©◊ë◊ë◊ß◊ö◊ß◊®◊©◊ê◊ù◊® ◊¶◊©◊ò◊†◊ß ◊ë◊©◊ï◊ì◊ß◊ì ◊ü◊ì◊ì◊ï◊ß◊ì?\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\") #\"bf16\") #bf16\") # fp16 # orig used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H1bosQnEwXvf"
   },
   "outputs": [],
   "source": [
    "num_epochs = 3#4\n",
    "bch_size = 16#36#32#8#3#2\n",
    "\n",
    "# opt = Adafactor(3e-4)##AdamWeightDecay(1e-4) #default: AdamWeightDecay(2e-5)\n",
    "# opt = AdamWeightDecay(5e-4)#(1e-3)\n",
    "# opt = Adam(8e-4)\n",
    "\n",
    "if FAST_RUN:\n",
    "    num_epochs = 2\n",
    "    # bch_size = 16\n",
    "    bch_size = 32\n",
    "    MAX_LEN = int(MAX_LEN//1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dEGNJ6xlxQvC"
   },
   "outputs": [],
   "source": [
    "# model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t30_150M_UR50D\"\n",
    "# model_checkpoint =  \"facebook/esm2_t33_650M_UR50D\"\n",
    "models_list = [\"facebook/esm2_t6_8M_UR50D\",\"facebook/esm2_t12_35M_UR50D\",\"facebook/esm2_t30_150M_UR50D\",\"facebook/esm2_t33_650M_UR50D\"]\n",
    "if FAST_RUN:\n",
    "#     model_checkpoint =\"facebook/esm2_t6_8M_UR50D\"\n",
    "  # model_checkpoint =  \"facebook/esm2_t12_35M_UR50D\"\n",
    "    models_list = [\"facebook/esm2_t6_8M_UR50D\",\"facebook/esm2_t12_35M_UR50D\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4653,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "Eggiq94CubAw",
    "outputId": "9ab29551-cfb8-47d0-9b6d-ff6457e3ca09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20340\n",
      "Sequence        20340\n",
      "virus               2\n",
      "Length           1454\n",
      "Cluster name    16810\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>virus</th>\n",
       "      <th>Length</th>\n",
       "      <th>Cluster name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>Cluster: DET1- and DDB1-associated protein 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...</td>\n",
       "      <td>1</td>\n",
       "      <td>362</td>\n",
       "      <td>Cluster: Protein MGF 360-19R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>Cluster: Pyrin domain-containing protein 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>Cluster: Protein FAM9B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...</td>\n",
       "      <td>0</td>\n",
       "      <td>1360</td>\n",
       "      <td>Cluster: TRAF2 and NCK-interacting protein kinase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20335</th>\n",
       "      <td>MDPDKQDALNSIENSIYRTAFKLQSVQTLCQLDLIDSSLIQQVLLR...</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>Cluster: Dystrotelin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20336</th>\n",
       "      <td>MLCPWRTANLGLLLILTIFLVAEAEGAAQPNNSLMLQTSKENHALA...</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>Cluster: Cell surface glycoprotein CD200 recep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>MCLRFFSPVPGSTSSATNVTMVVSAGPWSSEKAEMNILEINEKLRP...</td>\n",
       "      <td>0</td>\n",
       "      <td>421</td>\n",
       "      <td>Cluster: Putative neuroblastoma breakpoint fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20338</th>\n",
       "      <td>MASHAGQQHAPAFGQAARASGPTDGRAASRPSHRQGASEARGDPEL...</td>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>Cluster: Thymidine kinase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20339</th>\n",
       "      <td>MSASSLLEQRPKGQGNKVQNGSVHQKDGLNDDDFEPYLSPQARPNN...</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "      <td>Cluster: YTH domain-containing family protein 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20340 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sequence  virus  Length  \\\n",
       "0      MADFLKGLPVYNKSNFSRFHADSVCKASNRRPSVYLPTREYPSEQI...      0     102   \n",
       "1      MPSTLQVLAKKVLALEHKENDHISREYYYHILKCCGLWWHEAPIIL...      1     362   \n",
       "2      MASSAELDFNLQALLEQLSQDELSKFKSLIRTISLGKELQTVPQTE...      0      97   \n",
       "3      MAAWGKKHAGKDPVRDECEERNRFTETREEDVTDEHGEREPFAETD...      0     186   \n",
       "4      MASDSPARSLDEIDLSALRDPAGIFELVELVGNGTYGQVYKGRHVK...      0    1360   \n",
       "...                                                  ...    ...     ...   \n",
       "20335  MDPDKQDALNSIENSIYRTAFKLQSVQTLCQLDLIDSSLIQQVLLR...      0     578   \n",
       "20336  MLCPWRTANLGLLLILTIFLVAEAEGAAQPNNSLMLQTSKENHALA...      0     348   \n",
       "20337  MCLRFFSPVPGSTSSATNVTMVVSAGPWSSEKAEMNILEINEKLRP...      0     421   \n",
       "20338  MASHAGQQHAPAFGQAARASGPTDGRAASRPSHRQGASEARGDPEL...      1     376   \n",
       "20339  MSASSLLEQRPKGQGNKVQNGSVHQKDGLNDDDFEPYLSPQARPNN...      0     579   \n",
       "\n",
       "                                            Cluster name  \n",
       "0           Cluster: DET1- and DDB1-associated protein 1  \n",
       "1                           Cluster: Protein MGF 360-19R  \n",
       "2             Cluster: Pyrin domain-containing protein 2  \n",
       "3                                 Cluster: Protein FAM9B  \n",
       "4      Cluster: TRAF2 and NCK-interacting protein kinase  \n",
       "...                                                  ...  \n",
       "20335                               Cluster: Dystrotelin  \n",
       "20336  Cluster: Cell surface glycoprotein CD200 recep...  \n",
       "20337  Cluster: Putative neuroblastoma breakpoint fam...  \n",
       "20338                          Cluster: Thymidine kinase  \n",
       "20339    Cluster: YTH domain-containing family protein 2  \n",
       "\n",
       "[20340 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_parquet(DATA_PATH) # numpy to pandas\n",
    "df = pd.read_csv(DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\"Cluster name\"])\n",
    "df_test = pd.read_csv(TEST_DATA_PATH,usecols=[\"Sequence\",\"virus\",\"Length\",\"Cluster name\"])\n",
    " ## lengths of all the seqs\n",
    "\n",
    "## concat all data - for CV\n",
    "# df = pd.concat([df,df_test])\n",
    "print(df.shape[0])\n",
    "print(df.nunique())\n",
    "\n",
    "if FAST_RUN:\n",
    "#   # df.loc[df[\"Length\"]>200]\n",
    "    df = df.sample(frac=0.01)\n",
    "    df_test = df_test.sample(frac=0.02)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Cj3aknNQvHKH"
   },
   "outputs": [],
   "source": [
    "# df = df.loc[df[\"Length\"]>500].sample(500)\n",
    "# print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ivYZ9IhmvHKH"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df.groupby([\"virus\"])[\"Cluster name\"].nunique().describe().round(2)\n",
    "except:()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "fkeYBgfafVoy",
    "outputId": "e8f72139-fcad-41d6-f9df-7fb73571df63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14668.0</td>\n",
       "      <td>473.16</td>\n",
       "      <td>308.52</td>\n",
       "      <td>11.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>1533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5672.0</td>\n",
       "      <td>380.31</td>\n",
       "      <td>298.35</td>\n",
       "      <td>11.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>1520.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count    mean     std   min    25%    50%    75%     max\n",
       "virus                                                            \n",
       "0      14668.0  473.16  308.52  11.0  244.0  398.0  623.0  1533.0\n",
       "1       5672.0  380.31  298.35  11.0  151.0  297.0  514.0  1520.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"virus\"])[\"Length\"].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "rm2Ms4BmQ4wn",
    "outputId": "a8edf73e-0541-43dd-8537-10b64de09661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean         0.28\n",
      "sum       5672.00\n",
      "count    20340.00\n",
      "Name: virus, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    20340.0\n",
       "mean       447.0\n",
       "std        309.0\n",
       "min         11.0\n",
       "25%        213.0\n",
       "50%        372.0\n",
       "75%        592.0\n",
       "max       1533.0\n",
       "Name: Length, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"virus\"].agg([\"mean\",\"sum\",\"count\"]).round(2))\n",
    "df[\"Length\"].describe().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1706644670770,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "F8VNnHUwRQRm",
    "outputId": "c5502a31-ec5d-417d-fb9e-87e1de7d358d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    20340.0\n",
      "mean       447.3\n",
      "std        308.5\n",
      "min         11.0\n",
      "25%        213.0\n",
      "50%        372.0\n",
      "75%        592.2\n",
      "max       1533.0\n",
      "Name: Length, dtype: float64\n",
      "mean         0.28\n",
      "sum       5672.00\n",
      "count    20340.00\n",
      "Name: virus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# df = df.loc[df[\"Length\"]<=2*MAX_LEN].reset_index(drop=True)\n",
    "# df_test = df_test.loc[df_test[\"Length\"]<=2*MAX_LEN].reset_index(drop=True)\n",
    "\n",
    "print(df[\"Length\"].describe().round(1))\n",
    "print(df[\"virus\"].agg([\"mean\",\"sum\",\"count\"]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aYtBRneFvHKI",
    "outputId": "37610c07-404c-4753-e41c-bd4c516c6102"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence        20340\n",
       "virus               2\n",
       "Length           1454\n",
       "Cluster name    16810\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PcMwJBkUov07"
   },
   "outputs": [],
   "source": [
    "# ## metadata about all sequences, can be used to identify and to define targets/labels\n",
    "# df_meta = pd.read_excel(METADATA_PATH).dropna(how=\"all\",axis=1)\n",
    "# df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSXy_OzjhIB7"
   },
   "source": [
    "### example pretrained fb/torch:\n",
    "* https://github.com/facebookresearch/esm#getting-started-with-this-repo-\n",
    "\n",
    "* Transformers + trainer example : (mlm case): https://github.com/facebookresearch/esm/discussions/556\n",
    "* keras models supported / via HF?\n",
    "  * https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "\n",
    "\n",
    "  TF finetuning example (sequence evel?):\n",
    "  https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb#scrollTo=4b26b828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CDdMQ98rii9t"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers import Adafactor, Adam # more memory effecient than adamWD\n",
    "import tensorflow\n",
    "# from tensorflow.keras.metrics.AUC()\n",
    "# from transformers import AutoTokenizer #DataCollatorForLanguageModeling,\n",
    "## https://huggingface.co/docs/transformers/model_doc/esm#transformers.EsmForSequenceClassification.forward.example\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification, Trainer\n",
    "from transformers import TFAutoModelForTokenClassification, TFAutoModelForSequenceClassification ,TFEsmForSequenceClassification\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tcssXXfaVYns"
   },
   "outputs": [],
   "source": [
    "ID2LABEL = {\n",
    "    0: \"Human\",\n",
    "    1: \"Virus\"\n",
    "}\n",
    "LABEL2ID = {v: k for k, v in ID2LABEL.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rR9-cKgPjzW2"
   },
   "outputs": [],
   "source": [
    "train_sequences = df[\"Sequence\"].tolist()\n",
    "train_labels = df[\"virus\"].tolist()\n",
    "train_groups = df[\"Cluster name\"].tolist()\n",
    "\n",
    "# # train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)#,stratify=labels)\n",
    "\n",
    "test_sequences = df_test[\"Sequence\"].tolist()\n",
    "test_labels = df_test[\"virus\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PP-a4bfgvHKP"
   },
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(base_model_name_or_path=model_checkpoint,\n",
    "# #                         init_lora_weights=\"loftq\", loftq_config=loftq_config,\n",
    "#     task_type= \"SEQ_CLS\",#TaskType.SEQ_CLS, ## disabling helps?? (then get \"ValueError: Attempting to unscale FP16 gradients.\")\n",
    "#     inference_mode=False, r= 2 if FAST_RUN else 4, #16,\n",
    "#     lora_alpha=8,\n",
    "# #     lora_dropout=0.05,\n",
    "#     use_rslora = True,\n",
    "#     bias= \"all\"#\"lora_only\",#\"none\",#\"lora_only\",#\"none\",#\"all\",\n",
    "#     # target_modules=[\n",
    "#     #     \"query\", \"key\", \"value\",\n",
    "#     #                 \"EsmSelfOutput.dense\",\n",
    "#     #         \"EsmIntermediate.dense\",\n",
    "#     #         \"EsmOutput.dense\",\n",
    "#     #                 # \"word_embeddings\",\n",
    "#     #                 # \"EsmClassificationHead.dense\", ## not sure if works/changes anything\n",
    "#     #                 # \"out_proj\",\n",
    "#     #                 # \"classifier\" # fails\n",
    "#     #                 ],\n",
    "# #         target_modules=  target_modules#\"all-linear\"#modules_list,\n",
    "#            ,target_modules=  \"all-linear\"\n",
    "\n",
    "#     # # ### https://www.kaggle.com/code/andregrbnr/protein-sequence-classification\n",
    "# #      ,modules_to_save= \"all-linear\",\n",
    "# #                          [#\"decode_head\",\n",
    "# # #                       \"classifier\",\n",
    "# #          \"EsmClassificationHead\",\n",
    "# #          'classifier.dense', 'classifier.out_proj',\n",
    "# #          \"pooler\",\n",
    "# # # #                      'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "# #                      ]\n",
    "#     ## 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "#     # modules_to_save=[\"classifier\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mEoRinaIznOa"
   },
   "outputs": [],
   "source": [
    "##https://huggingface.co/docs/peft/main/en/developer_guides/quantization\n",
    "## lotfQ config - for this, do not initialize as quantized!\n",
    "# from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "# loftq_config = LoftQConfig(loftq_bits=4)\n",
    "\n",
    "## https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True, # disable to train ok;\n",
    "#   load_in_8bit=True, # alt\n",
    "# load_in_4bit=False,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "  # llm_int8_has_fp16_weight= True, # try alt\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 3984,
     "status": "ok",
     "timestamp": 1706644674751,
     "user": {
      "displayName": "Dan Ofer",
      "userId": "14537932808605235168"
     },
     "user_tz": -120
    },
    "id": "k_KUug1Lh5bH",
    "outputId": "14e7d50d-cdfe-4965-9f83-704a4a821e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load ESM-2 model\n",
    "## smallest: esm2_t6_8M_UR50D\n",
    "## 2d smallest\n",
    "## large: esm2_t33_650M_UR50D\n",
    "\n",
    " # ElnaggarLab/ankh-base\n",
    " ### https://github.com/agemagician/Ankh/blob/main/examples/binary_classification_solubility_task.ipynb - different model?\n",
    " #  https://github.com/agemagician/Ankh#models   - 450M model size # model_checkpoint =   \"ElnaggarLab/ankh-base\"\n",
    "\n",
    "model_max_len = min(1024,MAX_LEN) # 800\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "# #                                           padding= False#True # orig\n",
    "#                                           padding= True# alt\n",
    "#                                           ,truncation=True,max_length=model_max_len)\n",
    "\n",
    "num_labels = 2 # max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
    "print(\"Num labels:\", num_labels)\n",
    "##ORIG:\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\") # worked, orig\n",
    "\n",
    "## try this now, alt:\n",
    "# model = EsmForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "# model = TFEsmForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels,problem_type=\"single_label_classification\")\n",
    "\n",
    "## https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=fc164b49 # uses trainer\n",
    "# ##\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, #num_labels=num_labels,\n",
    "# #                                                            problem_type=\"single_label_classification\", # was enabled\n",
    "# #                                                            load_in_4bit=True, # disable to train ok\n",
    "# #                                                            load_in_4bit= False,\n",
    "# #                                                             quantization_config=nf4_config,\n",
    "#                                                           #  load_in_8bit=True,  torch_dtype=torch.float32, # try this - new\n",
    "# #                                                            device_map= \"cuda:0\",#\"auto\",\n",
    "#                                                            device_map=\"auto\",\n",
    "#                                                           num_labels=len(ID2LABEL), id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "# #                                                             trust_remote_code=True,\n",
    "#                                                           )\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # prepares the whole model for kbit training\n",
    "\n",
    "# last_layer_num = model.num_layers ## 33 for esm2_t33_650M_UR50D\n",
    "# print(last_layer_num )\n",
    "\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uHrUC-c9vHKQ"
   },
   "outputs": [],
   "source": [
    "def get_raw_model_lora(model_checkpoint):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                           problem_type=\"single_label_classification\", # was enabled\n",
    "#                                                            load_in_4bit=True,\n",
    "#                                                             quantization_config=nf4_config,\n",
    "                                                          #  load_in_8bit=True,  torch_dtype=torch.float32, # try this - new\n",
    "                                                           device_map=\"auto\",\n",
    "                                                          num_labels=len(ID2LABEL), id2label=ID2LABEL, label2id=LABEL2ID,\n",
    "                                                          )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) # prepares the whole model for kbit training\n",
    "    peft_config = LoraConfig(base_model_name_or_path=model_checkpoint,\n",
    "    task_type= \"SEQ_CLS\",inference_mode=False, r= 2 if FAST_RUN else 8, #16,\n",
    "    lora_alpha=8,use_rslora = True, bias= \"all\"  ,target_modules=  \"all-linear\"\n",
    "    # # ### https://www.kaggle.com/code/andregrbnr/protein-sequence-classification\n",
    "#      ,modules_to_save= \"all-linear\",\n",
    "#                          [#\"decode_head\",\n",
    "# #                       \"classifier\",\n",
    "#          \"EsmClassificationHead\",\n",
    "#          'classifier.dense', 'classifier.out_proj',\n",
    "#          \"pooler\",\n",
    "# # #                      'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'\n",
    "#                      ]\n",
    ")\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LHVOJZO7vHKQ"
   },
   "outputs": [],
   "source": [
    "# model = get_raw_model_lora(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EID_uE2vHKR"
   },
   "source": [
    "\"After we wrap our base model model with PeftModel along with the config, we get a new model where only the LoRA parameters are trainable (so-called ‚Äúupdate matrices‚Äù) while the pre-trained parameters are kept frozen. These include the parameters of the randomly initialized classifier parameters too. This is NOT we want when fine-tuning the base model on our custom dataset. To ensure that the classifier parameters are also trained, we specify modules_to_save. This also ensures that these modules are serialized alongside the LoRA trainable parameters when using utilities like save_pretrained() and push_to_hub().\n",
    "\n",
    "In addition to specifying the target_modules within LoraConfig, we also need to specify the modules_to_save. When we wrap our base model with PeftModel and pass the configuration, we obtain a new model in which only the LoRA parameters are trainable, while the pre-trained parameters and the randomly initialized classifier parameters are kept frozen. However, we do want to train the classifier parameters. By specifying the modules_to_save argument, we ensure that the classifier parameters are also trainable, and they will be serialized alongside the LoRA trainable parameters when we use utility functions like save_pretrained() and push_to_hub().\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Aw4wKUDNFnBj",
    "outputId": "84c37558-0d46-4e41-a8ab-e481f2183608"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20340\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Warning - longer than allowed length - 1024\n",
    "# \"\"\"\n",
    "\n",
    "# same tokenizer for all?\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\",\n",
    "#                                           padding= False#True # orig\n",
    "                                          padding= True# alt\n",
    "                                          ,truncation=True,max_length=model_max_len)\n",
    "\n",
    "train_tokenized = tokenizer(train_sequences,  truncation=True,max_length=model_max_len,padding=True) # padding=True,\n",
    "test_tokenized = tokenizer(test_sequences,  truncation=True,max_length=model_max_len,padding=True) # padding=True,\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "y_4o9iEZFLBZ"
   },
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "# # # Use the accelerator\n",
    "# # ### try disabling? (with qlora)\n",
    "# # accelerator = Accelerator()# trying this\n",
    "# # # # accelerator = Accelerator(mixed_precision=\"fp16\") # fp16 # orig used\n",
    "# model = accelerator.prepare(model)\n",
    "\n",
    "train_dataset = accelerator.prepare(train_dataset)\n",
    "test_dataset = accelerator.prepare(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEcix1ZWfQg-"
   },
   "source": [
    "With 35M model :\n",
    "```\n",
    "Default (1024) max length\n",
    "batch_size=8\n",
    "\n",
    "opt = Adafactor(1e-4)##AdamWeightDecay(1e-4) #default: AdamWeightDecay(2e-5)\n",
    "model.compile(optimizer=opt, metrics=[\"accuracy\"],\n",
    "              loss=\"BinaryCrossentropy\")\n",
    "3813/3813 [==============================] - 2625s 661ms/step - loss: 0.2452 - accuracy: 0.9104 - val_loss: 0.1622 - val_accuracy: 0.9363\n",
    "Epoch 2/3\n",
    "3813/3813 [==============================] - 2518s 661ms/step - loss: 0.1218 - accuracy: 0.9597 - val_loss: 0.1533 - val_accuracy: 0.9463\n",
    "Epoch 3/3\n",
    "3813/3813 [==============================] - 2523s 662ms/step - loss: 0.0730 - accuracy: 0.9799 - val_loss: 0.1978 - val_accuracy: 0.9436\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Q_3tSKYxJJtE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,roc_auc_score\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_metric\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"roc_auc\":roc_auc_score(labels,pred.predictions[:,1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hbj58ofX8umQ"
   },
   "outputs": [],
   "source": [
    "# if TRAIN_MODEL:\n",
    "#     result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "JusvkE07vHKT",
    "outputId": "45a244ec-bc2a-49a3-a0e0-fd93ddff9e27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['facebook/esm2_t6_8M_UR50D',\n",
       " 'facebook/esm2_t12_35M_UR50D',\n",
       " 'facebook/esm2_t30_150M_UR50D',\n",
       " 'facebook/esm2_t33_650M_UR50D']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "282532c2d90a4eee99c69fb5fc9c0a41"
     ]
    },
    "id": "T8bwYd8pvHKU",
    "outputId": "47fb3bf6-fe40-403c-80ea-1025b09a3055",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t6_8M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 409,947 || all params: 8,236,965 || trainable%: 4.976918076014648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6abe5210a6f4c51adee31c492d5a49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111226627777771, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/Protein-Virus anom copy/wandb/run-20240312_094615-dtenx9yi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ddofer/humVir_evalDLTest/runs/dtenx9yi' target=\"_blank\">esm2_t6_8M_UR50D-finetuned-ft-humVir</a></strong> to <a href='https://wandb.ai/ddofer/humVir_evalDLTest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ddofer/humVir_evalDLTest' target=\"_blank\">https://wandb.ai/ddofer/humVir_evalDLTest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ddofer/humVir_evalDLTest/runs/dtenx9yi' target=\"_blank\">https://wandb.ai/ddofer/humVir_evalDLTest/runs/dtenx9yi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3816' max='3816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3816/3816 26:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.238801</td>\n",
       "      <td>0.904961</td>\n",
       "      <td>0.872114</td>\n",
       "      <td>0.846767</td>\n",
       "      <td>0.912599</td>\n",
       "      <td>0.972365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.170099</td>\n",
       "      <td>0.937827</td>\n",
       "      <td>0.909333</td>\n",
       "      <td>0.903238</td>\n",
       "      <td>0.915856</td>\n",
       "      <td>0.978907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.203130</td>\n",
       "      <td>0.947247</td>\n",
       "      <td>0.921523</td>\n",
       "      <td>0.923343</td>\n",
       "      <td>0.919735</td>\n",
       "      <td>0.980877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='598' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [598/598 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t12_35M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,138,587 || all params: 35,080,125 || trainable%: 3.2456754358771525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3816' max='3816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3816/3816 58:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.163050</td>\n",
       "      <td>0.944107</td>\n",
       "      <td>0.917984</td>\n",
       "      <td>0.914150</td>\n",
       "      <td>0.921977</td>\n",
       "      <td>0.982626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.140721</td>\n",
       "      <td>0.949969</td>\n",
       "      <td>0.926070</td>\n",
       "      <td>0.925027</td>\n",
       "      <td>0.927125</td>\n",
       "      <td>0.985405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.184896</td>\n",
       "      <td>0.958342</td>\n",
       "      <td>0.938138</td>\n",
       "      <td>0.939233</td>\n",
       "      <td>0.937054</td>\n",
       "      <td>0.986867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-1272 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /facebook/esm2_t12_35M_UR50D/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fc22ab75030>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: c14b7ae7-a575-40a8-8480-009d9b4831c2)') - silently ignoring the lookup for the file config.json in facebook/esm2_t12_35M_UR50D.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in facebook/esm2_t12_35M_UR50D - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-2544 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /facebook/esm2_t12_35M_UR50D/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fc2104ca200>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: b5833195-4773-48b5-973d-154bb4513bd9)') - silently ignoring the lookup for the file config.json in facebook/esm2_t12_35M_UR50D.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in facebook/esm2_t12_35M_UR50D - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-3816 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /facebook/esm2_t12_35M_UR50D/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fc2104cbe20>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: e05fd72f-58e0-4910-a777-395ca7041ed5)') - silently ignoring the lookup for the file config.json in facebook/esm2_t12_35M_UR50D.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in facebook/esm2_t12_35M_UR50D - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='598' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [598/598 01:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t30_150M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,408,347 || all params: 152,008,645 || trainable%: 2.242206027163784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3816' max='3816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3816/3816 2:43:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.105484</td>\n",
       "      <td>0.962110</td>\n",
       "      <td>0.944933</td>\n",
       "      <td>0.936946</td>\n",
       "      <td>0.953595</td>\n",
       "      <td>0.990742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.098933</td>\n",
       "      <td>0.965041</td>\n",
       "      <td>0.948342</td>\n",
       "      <td>0.947244</td>\n",
       "      <td>0.949452</td>\n",
       "      <td>0.992541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.120590</td>\n",
       "      <td>0.969856</td>\n",
       "      <td>0.955409</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.956053</td>\n",
       "      <td>0.992572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-1272 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-2544 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /facebook/esm2_t30_150M_UR50D/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fc2105c1a20>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 963287ef-9b97-4a96-891d-5caf3a46dfd3)') - silently ignoring the lookup for the file config.json in facebook/esm2_t30_150M_UR50D.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in facebook/esm2_t30_150M_UR50D - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-3816 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='598' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [598/598 03:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2_t33_650M_UR50D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,226,747 || all params: 660,148,065 || trainable%: 1.2461972451589327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3816' max='3816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3816/3816 4:41:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.075292</td>\n",
       "      <td>0.971740</td>\n",
       "      <td>0.958387</td>\n",
       "      <td>0.955699</td>\n",
       "      <td>0.961142</td>\n",
       "      <td>0.995090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.090054</td>\n",
       "      <td>0.974042</td>\n",
       "      <td>0.962364</td>\n",
       "      <td>0.953097</td>\n",
       "      <td>0.972507</td>\n",
       "      <td>0.995853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.085569</td>\n",
       "      <td>0.978648</td>\n",
       "      <td>0.968482</td>\n",
       "      <td>0.966849</td>\n",
       "      <td>0.970138</td>\n",
       "      <td>0.996657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoint-1272 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-2544 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /facebook/esm2_t33_650M_UR50D/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fc1d567b8b0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: edf595bf-69ff-42db-9ab4-2bc7cae66f82)') - silently ignoring the lookup for the file config.json in facebook/esm2_t33_650M_UR50D.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in facebook/esm2_t33_650M_UR50D - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory ./checkpoint-3816 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /facebook/esm2_t33_650M_UR50D/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fc1d4de71f0>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 75dddedb-5ef6-48b7-8fbf-aaa4e2ac8aad)') - silently ignoring the lookup for the file config.json in facebook/esm2_t33_650M_UR50D.\n",
      "  warnings.warn(\n",
      "/home/ddofer/anaconda3/envs/hf/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in facebook/esm2_t33_650M_UR50D - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1196' max='598' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [598/598 11:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16903.0131, 'train_samples_per_second': 3.61, 'train_steps_per_second': 0.226, 'total_flos': 2.469858576132096e+17, 'train_loss': 0.06412847974265397, 'epoch': 3.0, 'eval_loss': 0.0855686143040657, 'eval_accuracy': 0.9786476868327402, 'eval_f1': 0.968481618902723, 'eval_precision': 0.9668493030595249, 'eval_recall': 0.9701380071405388, 'eval_roc_auc': 0.996656799740344, 'eval_runtime': 335.5286, 'eval_samples_per_second': 14.237, 'eval_steps_per_second': 1.782}\n"
     ]
    }
   ],
   "source": [
    "result_eval_dict = {}\n",
    "\n",
    "## could log models vs run properly, skip for now\n",
    "## https://docs.wandb.ai/guides/track/log/log-models\n",
    "\n",
    "for model_checkpoint in models_list:\n",
    "    # ### AttributeError: 'TFEsmForSequenceClassification' object has no attribute 'to'\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    print(model_name)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        run_name=f\"{model_name}-finetuned-ft-humVir\",\n",
    "    #     f\"/content/drive/MyDrive/proteins/New Protein-Virus anom project/t{model_name}-finetuned-humVir\",\n",
    "        per_device_train_batch_size=bch_size,\n",
    "        overwrite_output_dir=True,\n",
    "        # per_device_eval_batch_size=int(2*bch_size),\n",
    "        # gradient_accumulation_steps= 2, #4,\n",
    "        gradient_checkpointing=True, # maybe causes slowdown?\n",
    "        # fp16=True,\n",
    "        bf16=True, # needs ampere, not supported\n",
    "        tf32=True,\n",
    "            torch_compile = True,\n",
    "        optim = \"adamw_8bit\",#\"paged_adamw_8bit\", # adamw_bnb_8bit\n",
    "    #     optim=\"paged_adamw_32bit\",\n",
    "        label_names = [\"labels\"],\n",
    "        learning_rate = 5e-4,#5e-4, #5e-3,\n",
    "        # lr_scheduler_type=\"cosine\",\n",
    "        max_grad_norm = 0.95,\n",
    "    #     weight_decay=0.001,\n",
    "        # eval_accumulation_steps = 2#8\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        # warmup_ratio=0.02,\n",
    "        save_strategy= \"epoch\",#\"no\",\n",
    "        output_dir=\".\",\n",
    "         no_cuda=False,\n",
    "         greater_is_better=True,\n",
    "         # save_total_limit=1,\n",
    "      remove_unused_columns=False,\n",
    "        auto_find_batch_size = True, # new , reduces if oom\n",
    "        num_train_epochs=num_epochs,\n",
    "        # load_best_model_at_end=True,\n",
    "        metric_for_best_model= \"eval_roc_auc\",#\"accuracy\",\n",
    "        group_by_length=True,\n",
    "\n",
    "        report_to=\"wandb\",  # enable logging to W&B\n",
    "        # run_name=\"bert-base-high-lr\",  # name of the W&B run (optional)\n",
    "        # logging_steps=1,  # how often to log to W&B\n",
    "    )\n",
    "\n",
    "    model = get_raw_model_lora(model_checkpoint)\n",
    "    trainer = Trainer(model=model, args=training_args,\n",
    "                  train_dataset=train_dataset,eval_dataset=test_dataset,\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "    train_result = trainer.train()\n",
    "    result_eval_dict[model_name] =  train_result.metrics\n",
    "    result_eval_dict[model_name].update(trainer.evaluate())\n",
    "    # result_eval_dict[model_name] = trainer.evaluate()\n",
    "print(result_eval_dict[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uzpFkil2vHKV"
   },
   "outputs": [],
   "source": [
    "result_eval_dict[model_name] =  train_result.metrics\n",
    "result_eval_dict[model_name].update(trainer.evaluate())\n",
    "# result_eval_dict[model_name] = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "E9kfEVWQvHKV",
    "outputId": "e4dc0ae5-e732-45f3-a2c4-c732281b180b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_roc_auc</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>esm2_t6_8M_UR50D</th>\n",
       "      <td>1588.6776</td>\n",
       "      <td>38.409</td>\n",
       "      <td>2.402</td>\n",
       "      <td>2.961046e+15</td>\n",
       "      <td>0.157278</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.203130</td>\n",
       "      <td>0.947247</td>\n",
       "      <td>0.921523</td>\n",
       "      <td>0.923343</td>\n",
       "      <td>0.919735</td>\n",
       "      <td>0.980877</td>\n",
       "      <td>31.2410</td>\n",
       "      <td>152.908</td>\n",
       "      <td>19.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esm2_t12_35M_UR50D</th>\n",
       "      <td>3539.3568</td>\n",
       "      <td>17.240</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.296121e+16</td>\n",
       "      <td>0.109685</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.184896</td>\n",
       "      <td>0.958342</td>\n",
       "      <td>0.938138</td>\n",
       "      <td>0.939233</td>\n",
       "      <td>0.937054</td>\n",
       "      <td>0.986867</td>\n",
       "      <td>67.5681</td>\n",
       "      <td>70.699</td>\n",
       "      <td>8.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esm2_t30_150M_UR50D</th>\n",
       "      <td>9839.0309</td>\n",
       "      <td>6.202</td>\n",
       "      <td>0.388</td>\n",
       "      <td>5.673499e+16</td>\n",
       "      <td>0.077224</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.120590</td>\n",
       "      <td>0.969856</td>\n",
       "      <td>0.955409</td>\n",
       "      <td>0.954769</td>\n",
       "      <td>0.956053</td>\n",
       "      <td>0.992572</td>\n",
       "      <td>186.6819</td>\n",
       "      <td>25.589</td>\n",
       "      <td>3.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>esm2_t33_650M_UR50D</th>\n",
       "      <td>16903.0131</td>\n",
       "      <td>3.610</td>\n",
       "      <td>0.226</td>\n",
       "      <td>2.469859e+17</td>\n",
       "      <td>0.064128</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.085569</td>\n",
       "      <td>0.978648</td>\n",
       "      <td>0.968482</td>\n",
       "      <td>0.966849</td>\n",
       "      <td>0.970138</td>\n",
       "      <td>0.996657</td>\n",
       "      <td>335.2372</td>\n",
       "      <td>14.250</td>\n",
       "      <td>1.784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_runtime  train_samples_per_second  \\\n",
       "esm2_t6_8M_UR50D         1588.6776                    38.409   \n",
       "esm2_t12_35M_UR50D       3539.3568                    17.240   \n",
       "esm2_t30_150M_UR50D      9839.0309                     6.202   \n",
       "esm2_t33_650M_UR50D     16903.0131                     3.610   \n",
       "\n",
       "                     train_steps_per_second    total_flos  train_loss  epoch  \\\n",
       "esm2_t6_8M_UR50D                      2.402  2.961046e+15    0.157278    3.0   \n",
       "esm2_t12_35M_UR50D                    1.078  1.296121e+16    0.109685    3.0   \n",
       "esm2_t30_150M_UR50D                   0.388  5.673499e+16    0.077224    3.0   \n",
       "esm2_t33_650M_UR50D                   0.226  2.469859e+17    0.064128    3.0   \n",
       "\n",
       "                     eval_loss  eval_accuracy   eval_f1  eval_precision  \\\n",
       "esm2_t6_8M_UR50D      0.203130       0.947247  0.921523        0.923343   \n",
       "esm2_t12_35M_UR50D    0.184896       0.958342  0.938138        0.939233   \n",
       "esm2_t30_150M_UR50D   0.120590       0.969856  0.955409        0.954769   \n",
       "esm2_t33_650M_UR50D   0.085569       0.978648  0.968482        0.966849   \n",
       "\n",
       "                     eval_recall  eval_roc_auc  eval_runtime  \\\n",
       "esm2_t6_8M_UR50D        0.919735      0.980877       31.2410   \n",
       "esm2_t12_35M_UR50D      0.937054      0.986867       67.5681   \n",
       "esm2_t30_150M_UR50D     0.956053      0.992572      186.6819   \n",
       "esm2_t33_650M_UR50D     0.970138      0.996657      335.2372   \n",
       "\n",
       "                     eval_samples_per_second  eval_steps_per_second  \n",
       "esm2_t6_8M_UR50D                     152.908                 19.142  \n",
       "esm2_t12_35M_UR50D                    70.699                  8.850  \n",
       "esm2_t30_150M_UR50D                   25.589                  3.203  \n",
       "esm2_t33_650M_UR50D                   14.250                  1.784  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_eval = pd.DataFrame(result_eval_dict).T\n",
    "display(result_eval)\n",
    "if SAVE_OUTPUTS:\n",
    "    result_eval.to_csv(f\"DL_eval_{today}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "55T99gTMvHKW",
    "outputId": "21588e63-727b-46f9-8bb8-12130c8ee177",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/roc_auc</td><td>‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñà</td></tr><tr><td>train/train_loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñà</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97865</td></tr><tr><td>eval/f1</td><td>0.96848</td></tr><tr><td>eval/loss</td><td>0.08557</td></tr><tr><td>eval/precision</td><td>0.96685</td></tr><tr><td>eval/recall</td><td>0.97014</td></tr><tr><td>eval/roc_auc</td><td>0.99666</td></tr><tr><td>eval/runtime</td><td>335.2372</td></tr><tr><td>eval/samples_per_second</td><td>14.25</td></tr><tr><td>eval/steps_per_second</td><td>1.784</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>3816</td></tr><tr><td>train/grad_norm</td><td>0.00328</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0163</td></tr><tr><td>train/total_flos</td><td>2.469858576132096e+17</td></tr><tr><td>train/train_loss</td><td>0.06413</td></tr><tr><td>train/train_runtime</td><td>16903.0131</td></tr><tr><td>train/train_samples_per_second</td><td>3.61</td></tr><tr><td>train/train_steps_per_second</td><td>0.226</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">esm2_t6_8M_UR50D-finetuned-ft-humVir</strong> at: <a href='https://wandb.ai/ddofer/humVir_evalDLTest/runs/dtenx9yi' target=\"_blank\">https://wandb.ai/ddofer/humVir_evalDLTest/runs/dtenx9yi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240312_094615-dtenx9yi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eval.to_csv(f\"DL_eval_{today}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypJgmAhjvHKd"
   },
   "source": [
    "### run CV\n",
    "\n",
    "150M , full data, withgradient checkpoint enabled: 2.2 min per epoch (4.4 total), for 500 (long) samples.\n",
    "```\n",
    " [42/42 04:41, Epoch 2/2]\n",
    "Epoch\tTraining Loss\tValidation Loss\tAccuracy\tF1\tPrecision\tRecall\tRoc Auc\n",
    "1\tNo log\t0.344198\t0.868263\t0.868263\t0.868263\t0.868263\t0.938507\n",
    "2\tNo log\t0.255124\t0.880240\t0.880240\t0.880240\t0.880240\t0.958227\n",
    "```\n",
    "similar time when not using gradient checkpoint and double (32) batch szie - 4 min for. epochs?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1f6tWCmDttwj5GsdTavt7m-dIH1-2f1P5",
     "timestamp": 1706088386277
    },
    {
     "file_id": "18pa4xD0P5vwL-MX30nrkeye78W7403TZ",
     "timestamp": 1702551198722
    },
    {
     "file_id": "1uwM47KOjpzSn2TylouVCrTiGuskjQtEd",
     "timestamp": 1690279074762
    },
    {
     "file_id": "1_4meskHUbh7-dkjF-ajOT_wazp1vxzC6",
     "timestamp": 1630488037990
    },
    {
     "file_id": "1325niNI11d1AGvkudzFonUaYu-_IpRSv",
     "timestamp": 1629371463407
    },
    {
     "file_id": "1eeHOXM2csXDuY2ysmo-npc7dWtr9hcKJ",
     "timestamp": 1629281016353
    },
    {
     "file_id": "https://github.com/sacdallago/bio_embeddings/blob/develop/notebooks/embed_fasta_sequences.ipynb",
     "timestamp": 1629109653928
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4380199,
     "sourceId": 7529659,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
